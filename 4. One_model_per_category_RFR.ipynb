{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.svm import LinearSVR, SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Use this DF from here on with the filled Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muriel/.local/lib/python3.8/site-packages/numpy/lib/arraysetops.py:580: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\n",
    "    './data/Modelar_UH2021_filled_precio.txt', parse_dates=[1], index_col=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py:3062: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "df_est = pd.read_csv(\n",
    "    './data/Estimar_UH2021_filled_precio.txt', parse_dates=[1], index_col=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df=df.drop_duplicates()\n",
    "\n",
    "conditions = [ (df[\"estado\"] == 'No Rotura'), (df[\"estado\"] == 'Transito'), (df[\"estado\"] == 'Rotura') ]\n",
    "values = [1, 0, -1]\n",
    "df[\"estado_num\"] = np.select(conditions, values)\n",
    "\n",
    "df[\"weekday\"] = df[\"fecha\"].dt.weekday\n",
    "df[\"antiguedad\"] = df[\"antiguedad\"].astype('Int64')\n",
    "df[\"antiguedad_std\"] = df[\"antiguedad\"]-df[\"antiguedad\"].min()\n",
    "df[\"categoria_dos\"] = df[\"categoria_dos\"].astype(\"Int64\")\n",
    "\n",
    "df = df.drop(columns=[\"estado\", \"antiguedad\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cyclical features\n",
    "df['weekday_sin'] = np.sin(df.fecha.dt.weekday * (2*np.pi/7))\n",
    "df['weekday_cos'] = np.cos(df.fecha.dt.weekday * (2*np.pi/7))\n",
    "\n",
    "month_con = df[\"fecha\"].dt.month + (df[\"fecha\"].dt.day / df[\"fecha\"].dt.days_in_month)\n",
    "df['month_sin'] = np.sin((month_con-1) * (2*np.pi/12))\n",
    "df['month_cos'] = np.cos((month_con-1) * (2*np.pi/12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_est=df_est.drop_duplicates()\n",
    "\n",
    "conditions = [ (df_est[\"estado\"] == 'No Rotura'), (df_est[\"estado\"] == 'Transito') ]\n",
    "values = [1, 0]\n",
    "df_est[\"estado_num\"] = np.select(conditions, values)\n",
    "\n",
    "df_est[\"weekday\"] = df_est[\"fecha\"].dt.weekday\n",
    "df_est[\"antiguedad\"] = pd.to_numeric(df_est[\"antiguedad\"], errors='coerce') \n",
    "df_est[\"antiguedad\"] = df_est[\"antiguedad\"].astype('Int64')\n",
    "df_est[\"antiguedad_std\"] = df_est[\"antiguedad\"]-df_est[\"antiguedad\"].min()\n",
    "df_est[\"categoria_dos\"] = pd.to_numeric(df_est[\"categoria_dos\"], errors='coerce') \n",
    "df_est[\"categoria_dos\"] = df_est[\"categoria_dos\"].astype(\"Int64\")\n",
    "\n",
    "df_est = df_est.drop(columns=[\"estado\", \"antiguedad\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cyclical features\n",
    "df_est['weekday_sin'] = np.sin(df_est.fecha.dt.weekday * (2*np.pi/7))\n",
    "df_est['weekday_cos'] = np.cos(df_est.fecha.dt.weekday * (2*np.pi/7))\n",
    "\n",
    "month_con = df_est[\"fecha\"].dt.month + (df_est[\"fecha\"].dt.day / df_est[\"fecha\"].dt.days_in_month)\n",
    "df_est['month_sin'] = np.sin((month_con-1) * (2*np.pi/12))\n",
    "df_est['month_cos'] = np.cos((month_con-1) * (2*np.pi/12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only use data before the pattern change\n",
    "#df = df[df.fecha < datetime.datetime(2016,1,24)]\n",
    "#df_est = df_est[df_est.fecha < datetime.datetime(2016,1,24)]\n",
    "# Only use data after the pattern change\n",
    "#df = df[df.fecha > datetime.datetime(2016,1,25)]\n",
    "#df_est = df_est[df_est.fecha > datetime.datetime(2016,1,25)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unwanted columns\n",
    "df = df.drop(columns=[\"fecha\", \"id\", \"weekday\"])\n",
    "df_est = df_est.drop(columns=[\"fecha\", \"id\", \"weekday\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop nans\n",
    "df = df.dropna()\n",
    "df_est = df_est.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset in categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only 49714 samples for categoria_uno = B\n",
      "Only 487 samples for categoria_uno = D\n",
      "Only 82068 samples for categoria_uno = G\n",
      "Only 30777 samples for categoria_uno = I\n",
      "Only 44982 samples for categoria_uno = L\n",
      "Only 3900 samples for categoria_uno = N\n",
      "Only 2444 samples for categoria_uno = O\n"
     ]
    }
   ],
   "source": [
    "# Split dataset in categorias_uno and limit number of training samples per categoria_uno\n",
    "number_samples_desired = 100000\n",
    "\n",
    "list_categoria_uno = sorted( df[\"categoria_uno\"].unique() )\n",
    "data_cat = [None]*len(list_categoria_uno)\n",
    "data_est_cat = [None]*len(list_categoria_uno)\n",
    "\n",
    "for index in range(len(list_categoria_uno)):\n",
    "\n",
    "    number_samples = number_samples_desired\n",
    "    number_samples_available = len(df[df[\"categoria_uno\"] == list_categoria_uno[index]])\n",
    "    if number_samples_desired > number_samples_available:\n",
    "        number_samples = number_samples_available\n",
    "        print(f\"Only {number_samples_available} samples for categoria_uno = {list_categoria_uno[index]}\")\n",
    "        \n",
    "    # Modelar\n",
    "    data_cat[index] = df[df[\"categoria_uno\"] == list_categoria_uno[index]].sample(n=number_samples, random_state=0)\n",
    "    data_cat[index] = data_cat[index].drop(columns = \"categoria_uno\")\n",
    "    # Estimar\n",
    "    data_est_cat[index] = df_est[df_est[\"categoria_uno\"] == list_categoria_uno[index]]\n",
    "    data_est_cat[index] = data_est_cat[index].drop(columns = \"categoria_uno\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 11) (100000,)\n",
      "90000 90000 10000\n",
      "(90000, 11) (90000,) (10000, 11)\n"
     ]
    }
   ],
   "source": [
    "X = [None]*len(list_categoria_uno)\n",
    "y = [None]*len(list_categoria_uno)\n",
    "X_train = [None]*len(list_categoria_uno)\n",
    "X_test = [None]*len(list_categoria_uno)\n",
    "y_train = [None]*len(list_categoria_uno)\n",
    "y_test = [None]*len(list_categoria_uno)\n",
    "\n",
    "for index in range(len(list_categoria_uno)):\n",
    "    X[index] = data_cat[index][data_cat[index].columns.difference([\"unidades_vendidas\"])]\n",
    "    y[index] = data_cat[index][\"unidades_vendidas\"]\n",
    "\n",
    "    X_train[index], X_test[index], y_train[index], y_test[index] = train_test_split(\n",
    "                                                            X[index], y[index], test_size=0.10, random_state=0)\n",
    "\n",
    "print(X[0].shape, y[0].shape)\n",
    "print(len(X_train[0]), len(y_train[0]), len(X_test[0]))\n",
    "print(X_train[0].shape, y_train[0].shape, X_test[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalitzation of selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize some features of the train and test datasets with the Standard Scaler/Robust Scaler \n",
    "# Select which columns to use with the scaler\n",
    "\n",
    "selected_columns = [\n",
    "#    \"fecha\",\n",
    "#    \"id\",\n",
    "    \"antiguedad_std\",\n",
    "#    \"campaña\",\n",
    "#    \"categoria_uno\",\n",
    "    \"categoria_dos\",\n",
    "#    \"dia_atipico\",\n",
    "#    \"estado_num\",\n",
    "#    \"month_cos\",\n",
    "#    \"month_sin\",\n",
    "    \"precio\",\n",
    "    \"visitas\",\n",
    "#    \"weekday\",\n",
    "#    \"weekday_cos\",\n",
    "#    \"weekday_sin\",\n",
    "    ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I train the scaler with ALL training data available\n",
    "scaler = StandardScaler().fit(df.loc[:,selected_columns])\n",
    "\n",
    "X_train_scaled = [None]*len(list_categoria_uno)\n",
    "X_train_non_scaled = [None]*len(list_categoria_uno)\n",
    "X_test_scaled = [None]*len(list_categoria_uno)\n",
    "X_test_non_scaled = [None]*len(list_categoria_uno)\n",
    "\n",
    "for index in range(len(list_categoria_uno)):\n",
    "    X_train_scaled[index] = scaler.transform(X_train[index].loc[:, selected_columns])\n",
    "    X_train_non_scaled[index] = X_train[index][X_train[index].columns.difference(selected_columns)]\n",
    "    X_train_scaled[index] = np.concatenate([X_train_non_scaled[index], X_train_scaled[index]], axis=1)\n",
    "\n",
    "    X_test_scaled[index] = scaler.transform(X_test[index].loc[:, selected_columns])\n",
    "    X_test_non_scaled[index] = X_test[index][X_test[index].columns.difference(selected_columns)]\n",
    "    X_test_scaled[index] = np.concatenate([X_test_non_scaled[index], X_test_scaled[index]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train models and predict validation samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training:  2021-03-11 13:40:35.311363\n",
      "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
      "                      max_depth=4, max_features='auto', max_leaf_nodes=None,\n",
      "                      max_samples=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      n_estimators=100, n_jobs=None, oob_score=False,\n",
      "                      random_state=0, verbose=0, warm_start=False)\n",
      "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
      "                      max_depth=4, max_features='auto', max_leaf_nodes=None,\n",
      "                      max_samples=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      n_estimators=100, n_jobs=None, oob_score=False,\n",
      "                      random_state=0, verbose=0, warm_start=False)\n",
      "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
      "                      max_depth=4, max_features='auto', max_leaf_nodes=None,\n",
      "                      max_samples=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      n_estimators=100, n_jobs=None, oob_score=False,\n",
      "                      random_state=0, verbose=0, warm_start=False)\n",
      "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
      "                      max_depth=4, max_features='auto', max_leaf_nodes=None,\n",
      "                      max_samples=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      n_estimators=100, n_jobs=None, oob_score=False,\n",
      "                      random_state=0, verbose=0, warm_start=False)\n",
      "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
      "                      max_depth=4, max_features='auto', max_leaf_nodes=None,\n",
      "                      max_samples=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      n_estimators=100, n_jobs=None, oob_score=False,\n",
      "                      random_state=0, verbose=0, warm_start=False)\n",
      "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
      "                      max_depth=4, max_features='auto', max_leaf_nodes=None,\n",
      "                      max_samples=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      n_estimators=100, n_jobs=None, oob_score=False,\n",
      "                      random_state=0, verbose=0, warm_start=False)\n",
      "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
      "                      max_depth=4, max_features='auto', max_leaf_nodes=None,\n",
      "                      max_samples=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      n_estimators=100, n_jobs=None, oob_score=False,\n",
      "                      random_state=0, verbose=0, warm_start=False)\n",
      "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
      "                      max_depth=4, max_features='auto', max_leaf_nodes=None,\n",
      "                      max_samples=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      n_estimators=100, n_jobs=None, oob_score=False,\n",
      "                      random_state=0, verbose=0, warm_start=False)\n",
      "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
      "                      max_depth=4, max_features='auto', max_leaf_nodes=None,\n",
      "                      max_samples=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      n_estimators=100, n_jobs=None, oob_score=False,\n",
      "                      random_state=0, verbose=0, warm_start=False)\n",
      "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
      "                      max_depth=4, max_features='auto', max_leaf_nodes=None,\n",
      "                      max_samples=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      n_estimators=100, n_jobs=None, oob_score=False,\n",
      "                      random_state=0, verbose=0, warm_start=False)\n",
      "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
      "                      max_depth=4, max_features='auto', max_leaf_nodes=None,\n",
      "                      max_samples=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      n_estimators=100, n_jobs=None, oob_score=False,\n",
      "                      random_state=0, verbose=0, warm_start=False)\n",
      "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
      "                      max_depth=4, max_features='auto', max_leaf_nodes=None,\n",
      "                      max_samples=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      n_estimators=100, n_jobs=None, oob_score=False,\n",
      "                      random_state=0, verbose=0, warm_start=False)\n",
      "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
      "                      max_depth=4, max_features='auto', max_leaf_nodes=None,\n",
      "                      max_samples=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      n_estimators=100, n_jobs=None, oob_score=False,\n",
      "                      random_state=0, verbose=0, warm_start=False)\n",
      "End training:  2021-03-11 13:41:28.659336\n"
     ]
    }
   ],
   "source": [
    "print(\"Start training: \", datetime.datetime.now())\n",
    "\n",
    "predictor = [None]*len(list_categoria_uno)\n",
    "y_predicted = [None]*len(list_categoria_uno)\n",
    "\n",
    "for index in range(len(list_categoria_uno)):\n",
    "    predictor[index] = LinearSVR(random_state=0, tol=1e-5, max_iter=10000)\n",
    "#    predictor[index] = SVR(kernel='rbf', C=2., tol=1e-5, max_iter=100000)\n",
    "    predictor[index] = RandomForestRegressor(max_depth=4, random_state=0)\n",
    "    print(predictor[index])\n",
    "\n",
    "    predictor[index].fit(X_train_scaled[index], y_train[index])\n",
    "    y_predicted[index] = predictor[index].predict(X_test_scaled[index])\n",
    "\n",
    "print(\"End training: \", datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrica_atmira(y_test, y_predicted):\n",
    "    rmse = mean_squared_error(y_test, y_predicted, squared=False)\n",
    "    rrmse = rmse/y_test.mean()\n",
    "    # Si el valor és negatiu és que hi ha hagut més demanda de la prevista, si el valor és positiu compta com a CF\n",
    "    diferencia = y_predicted - y_test\n",
    "    CF = np.sum(diferencia >= 0)/len(y_test)\n",
    "    metrica_minimitzar = (0.7*rrmse) + (0.3*(1-CF))\n",
    "    print(\"rmse = \", rmse)\n",
    "    print(\"y_mean = \", y_test.mean())\n",
    "    print(\"rrmse = \", rrmse)\n",
    "    print(\"CF =\", CF)\n",
    "    return metrica_minimitzar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A 10000 10000 10000 10000\n",
      "B 4972 4972 14972 14972\n",
      "C 10000 10000 24972 24972\n",
      "D 49 49 25021 25021\n",
      "E 10000 10000 35021 35021\n",
      "F 10000 10000 45021 45021\n",
      "G 8207 8207 53228 53228\n",
      "H 10000 10000 63228 63228\n",
      "I 3078 3078 66306 66306\n",
      "K 10000 10000 76306 76306\n",
      "L 4499 4499 80805 80805\n",
      "N 390 390 81195 81195\n",
      "O 245 245 81440 81440\n"
     ]
    }
   ],
   "source": [
    "# Reconstruct joint y_test and y_predicted\n",
    "y_test_reconst = pd.Series()\n",
    "y_predicted_reconst = np.array([])\n",
    "for index in range(len(list_categoria_uno)):\n",
    "    y_test_reconst = y_test_reconst.append(y_test[index])\n",
    "    y_predicted_reconst = np.concatenate((y_predicted_reconst,y_predicted[index]))\n",
    "    print(list_categoria_uno[index], len(y_test[index]), len(y_predicted[index]), \n",
    "          len(y_test_reconst), len(y_predicted_reconst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse =  13.406939217886908\n",
      "y_mean =  3.6659012770137522\n",
      "rrmse =  3.657201382359161\n",
      "CF = 0.7744720039292731\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.6276993664726307"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrica_atmira(y_test_reconst, y_predicted_reconst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse =  11.67899295726826\n",
      "y_mean =  3.566853482786229\n",
      "rrmse =  3.274312503620215\n",
      "CF = 0.477982385908727\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.448624036761532"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrica_atmira(y_test_reconst, y_predicted_reconst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "144.4px",
    "left": "1166px",
    "right": "20px",
    "top": "121px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
