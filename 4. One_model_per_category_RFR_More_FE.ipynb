{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.svm import LinearSVR, SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Use this DF from here on with the filled Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muriel/.local/lib/python3.8/site-packages/numpy/lib/arraysetops.py:580: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\n",
    "    './data/Modelar_UH2021_filled_precio.txt', parse_dates=[1], index_col=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py:3062: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "df_est = pd.read_csv(\n",
    "    './data/Estimar_UH2021_filled_precio.txt', parse_dates=[1], index_col=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df=df.drop_duplicates()\n",
    "\n",
    "conditions = [ (df[\"estado\"] == 'No Rotura'), (df[\"estado\"] == 'Transito'), (df[\"estado\"] == 'Rotura') ]\n",
    "values = [1, 0, -1]\n",
    "df[\"estado_num\"] = np.select(conditions, values)\n",
    "\n",
    "df[\"weekday\"] = df[\"fecha\"].dt.weekday\n",
    "df[\"antiguedad\"] = df[\"antiguedad\"].astype('Int64')\n",
    "df[\"antiguedad_std\"] = df[\"antiguedad\"]-df[\"antiguedad\"].min()\n",
    "df[\"categoria_dos\"] = df[\"categoria_dos\"].astype(\"Int64\")\n",
    "\n",
    "df = df.drop(columns=[\"estado\", \"antiguedad\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cyclical features\n",
    "df['weekday_sin'] = np.sin(df.fecha.dt.weekday * (2*np.pi/7))\n",
    "df['weekday_cos'] = np.cos(df.fecha.dt.weekday * (2*np.pi/7))\n",
    "\n",
    "month_con = df[\"fecha\"].dt.month + (df[\"fecha\"].dt.day / df[\"fecha\"].dt.days_in_month)\n",
    "df['month_sin'] = np.sin((month_con-1) * (2*np.pi/12))\n",
    "df['month_cos'] = np.cos((month_con-1) * (2*np.pi/12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_est=df_est.drop_duplicates()\n",
    "\n",
    "conditions = [ (df_est[\"estado\"] == 'No Rotura'), (df_est[\"estado\"] == 'Transito') ]\n",
    "values = [1, 0]\n",
    "df_est[\"estado_num\"] = np.select(conditions, values)\n",
    "\n",
    "df_est[\"weekday\"] = df_est[\"fecha\"].dt.weekday\n",
    "df_est[\"antiguedad\"] = pd.to_numeric(df_est[\"antiguedad\"], errors='coerce') \n",
    "df_est[\"antiguedad\"] = df_est[\"antiguedad\"].astype('Int64')\n",
    "df_est[\"antiguedad_std\"] = df_est[\"antiguedad\"]-df_est[\"antiguedad\"].min()\n",
    "df_est[\"categoria_dos\"] = pd.to_numeric(df_est[\"categoria_dos\"], errors='coerce') \n",
    "df_est[\"categoria_dos\"] = df_est[\"categoria_dos\"].astype(\"Int64\")\n",
    "\n",
    "df_est = df_est.drop(columns=[\"estado\", \"antiguedad\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cyclical features\n",
    "df_est['weekday_sin'] = np.sin(df_est.fecha.dt.weekday * (2*np.pi/7))\n",
    "df_est['weekday_cos'] = np.cos(df_est.fecha.dt.weekday * (2*np.pi/7))\n",
    "\n",
    "month_con = df_est[\"fecha\"].dt.month + (df_est[\"fecha\"].dt.day / df_est[\"fecha\"].dt.days_in_month)\n",
    "df_est['month_sin'] = np.sin((month_con-1) * (2*np.pi/12))\n",
    "df_est['month_cos'] = np.cos((month_con-1) * (2*np.pi/12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiply x5 number of visits before 25/1/2021\n",
    "conditions = [ (df[\"fecha\"] >= datetime.datetime(2016,1,25)), (df[\"fecha\"] < datetime.datetime(2016,1,25)) ]\n",
    "values = [df[\"visitas\"], df[\"visitas\"]*5]\n",
    "df[\"visitas\"] = np.select(conditions, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding of \"categoria_dos\" (171 new features)\n",
    "list_categoria_dos = df[\"categoria_dos\"].unique()\n",
    "for categoria in list_categoria_dos:\n",
    "    df[\"categoria_dos_\"+str(categoria)] = (df[\"categoria_dos\"] == categoria).astype(int)\n",
    "    df_est[\"categoria_dos_\"+str(categoria)] = (df_est[\"categoria_dos\"] == categoria).astype(int)\n",
    "df = df.drop(columns=[\"categoria_dos\"])\n",
    "df_est = df_est.drop(columns=[\"categoria_dos\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (NO MILLORA) Add feature visits 1 day before \n",
    "# It only works if there are no missing entries and they are sorted (!!)\n",
    "#df = df.sort_values(by = ['categoria_uno','id', 'fecha']).reset_index(drop=True)\n",
    "\n",
    "# Create transitional previous day visits feature\n",
    "#first_element = pd.Series([0])\n",
    "#visits_day_before = first_element.append(df[\"visitas\"], ignore_index=True)\n",
    "#df[\"visitas_dia_antes\"] = visits_day_before\n",
    "\n",
    "# Select all the first entries from the stations and set the previous entries and the tendencies to 0\n",
    "#product_unique_ids = sorted(df[\"id\"].unique())\n",
    "\n",
    "#for product_id in product_unique_ids:\n",
    "#    first_id_index = df[df[\"id\"] == product_id].index[0]\n",
    "#    df.at[first_id_index, \"visitas_dia_antes\"] = 0        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (NO MILLORA) Add feature visits 7 days before \n",
    "# It only works if there are no missing entries and they are sorted (!!)\n",
    "#df = df.sort_values(by = ['categoria_uno','id', 'fecha']).reset_index(drop=True)\n",
    "\n",
    "# Create transitional 7-day previous visits feature\n",
    "#first_element = pd.Series([0, 0, 0, 0, 0, 0, 0])\n",
    "#visits_7_days_before = first_element.append(df[\"visitas\"], ignore_index=True)\n",
    "#df[\"visitas_7_dias_antes\"] = visits_7_days_before\n",
    "\n",
    "# Select all the first entries from the stations and set the previous entries and the tendencies to 0\n",
    "#product_unique_ids = sorted(df[\"id\"].unique())\n",
    "\n",
    "#for product_id in product_unique_ids:\n",
    "#    first_id_index = df[df[\"id\"] == product_id].index[0]\n",
    "#    for extra_day in range(7):\n",
    "#        df.at[first_id_index+extra_day, \"visitas_7_dias_antes\"] = 0        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only use data before the pattern change\n",
    "#df = df[df.fecha < datetime.datetime(2016,1,24)]\n",
    "#df_est = df_est[df_est.fecha < datetime.datetime(2016,1,24)]\n",
    "# Only use data after the pattern change\n",
    "#df = df[df.fecha > datetime.datetime(2016,1,25)]\n",
    "#df_est = df_est[df_est.fecha > datetime.datetime(2016,1,25)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unwanted columns\n",
    "df = df.drop(columns=[\"fecha\", \"id\", \"weekday\"])\n",
    "df_est = df_est.drop(columns=[\"fecha\", \"id\", \"weekday\"])\n",
    "#df = df.drop(columns=[\"fecha\", \"id\", \"weekday_sin\", \"weekday_cos\"])\n",
    "#df_est = df_est.drop(columns=[\"fecha\", \"id\", \"weekday_sin\", \"weekday_cos\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop nans\n",
    "df = df.dropna()\n",
    "df_est = df_est.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset in categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only 572666 samples for categoria_uno = A\n",
      "Only 49714 samples for categoria_uno = B\n",
      "Only 158800 samples for categoria_uno = C\n",
      "Only 487 samples for categoria_uno = D\n",
      "Only 140443 samples for categoria_uno = E\n",
      "Only 115777 samples for categoria_uno = F\n",
      "Only 82068 samples for categoria_uno = G\n",
      "Only 130910 samples for categoria_uno = H\n",
      "Only 30777 samples for categoria_uno = I\n",
      "Only 226254 samples for categoria_uno = K\n",
      "Only 44982 samples for categoria_uno = L\n",
      "Only 3900 samples for categoria_uno = N\n",
      "Only 2444 samples for categoria_uno = O\n"
     ]
    }
   ],
   "source": [
    "# Split dataset in categorias_uno and limit number of training samples per categoria_uno\n",
    "number_samples_desired = 1000000\n",
    "\n",
    "list_categoria_uno = sorted( df[\"categoria_uno\"].unique() )\n",
    "data_cat = [None]*len(list_categoria_uno)\n",
    "data_est_cat = [None]*len(list_categoria_uno)\n",
    "\n",
    "for index in range(len(list_categoria_uno)):\n",
    "\n",
    "    number_samples = number_samples_desired\n",
    "    number_samples_available = len(df[df[\"categoria_uno\"] == list_categoria_uno[index]])\n",
    "    if number_samples_desired > number_samples_available:\n",
    "        number_samples = number_samples_available\n",
    "        print(f\"Only {number_samples_available} samples for categoria_uno = {list_categoria_uno[index]}\")\n",
    "        \n",
    "    # Modelar\n",
    "    data_cat[index] = df[df[\"categoria_uno\"] == list_categoria_uno[index]].sample(n=number_samples, random_state=0)\n",
    "    data_cat[index] = data_cat[index].drop(columns = \"categoria_uno\")\n",
    "    # Estimar\n",
    "    data_est_cat[index] = df_est[df_est[\"categoria_uno\"] == list_categoria_uno[index]]\n",
    "    data_est_cat[index] = data_est_cat[index].drop(columns = \"categoria_uno\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(572666, 193) (572666,)\n",
      "515399 515399 57267\n",
      "(515399, 193) (515399,) (57267, 193)\n"
     ]
    }
   ],
   "source": [
    "X = [None]*len(list_categoria_uno)\n",
    "y = [None]*len(list_categoria_uno)\n",
    "X_train = [None]*len(list_categoria_uno)\n",
    "X_test = [None]*len(list_categoria_uno)\n",
    "y_train = [None]*len(list_categoria_uno)\n",
    "y_test = [None]*len(list_categoria_uno)\n",
    "\n",
    "for index in range(len(list_categoria_uno)):\n",
    "    X[index] = data_cat[index][data_cat[index].columns.difference([\"unidades_vendidas\"])]\n",
    "    y[index] = data_cat[index][\"unidades_vendidas\"]\n",
    "    \n",
    "    X_train[index], X_test[index], y_train[index], y_test[index] = train_test_split(\n",
    "                                                            X[index], y[index], test_size=0.10, random_state=0)\n",
    "\n",
    "print(X[0].shape, y[0].shape)\n",
    "print(len(X_train[0]), len(y_train[0]), len(X_test[0]))\n",
    "print(X_train[0].shape, y_train[0].shape, X_test[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (NO MILLORA) Use for training only data from typical days and without campaigns (test data is left untouched)\n",
    "#index_typical = [None]*len(list_categoria_uno)\n",
    "\n",
    "#for cat_index in range(len(list_categoria_uno)):\n",
    "#    index_typical[cat_index] = X_train[cat_index].loc[(X_train[cat_index][\"dia_atipico\"] == 0) & \n",
    "#                                                       (X_train[cat_index][\"campaña\"] == 0), ].index\n",
    "#    X_train[cat_index] = X_train[cat_index][X_train[cat_index].index.isin(index_typical[cat_index])]\n",
    "#    y_train[cat_index] = y_train[cat_index][y_train[cat_index].index.isin(index_typical[cat_index])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalitzation of selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize some features of the train and test datasets with the Standard Scaler/Robust Scaler \n",
    "# Select which columns to use with the scaler\n",
    "\n",
    "selected_columns = [\n",
    "#    \"fecha\",\n",
    "#    \"id\",\n",
    "    \"antiguedad_std\",\n",
    "#    \"campaña\",\n",
    "#    \"categoria_uno\",\n",
    "#    \"categoria_dos\",\n",
    "#    \"dia_atipico\",\n",
    "#    \"estado_num\",\n",
    "#    \"month_cos\",\n",
    "#    \"month_sin\",\n",
    "    \"precio\",\n",
    "    \"visitas\",\n",
    "#    \"weekday\",\n",
    "#    \"weekday_cos\",\n",
    "#    \"weekday_sin\",\n",
    "#    \"visitas_dia_antes\",\n",
    "#    \"visitas_7_dias_antes\",\n",
    "    ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I train the scaler with ALL training data available\n",
    "scaler = StandardScaler().fit(df.loc[:,selected_columns])\n",
    "#scaler = RobustScaler().fit(df.loc[:,selected_columns])\n",
    "\n",
    "X_train_scaled = [None]*len(list_categoria_uno)\n",
    "X_train_non_scaled = [None]*len(list_categoria_uno)\n",
    "X_test_scaled = [None]*len(list_categoria_uno)\n",
    "X_test_non_scaled = [None]*len(list_categoria_uno)\n",
    "\n",
    "for index in range(len(list_categoria_uno)):\n",
    "    X_train_scaled[index] = scaler.transform(X_train[index].loc[:, selected_columns])\n",
    "    X_train_non_scaled[index] = X_train[index][X_train[index].columns.difference(selected_columns)]\n",
    "    X_train_scaled[index] = np.concatenate([X_train_non_scaled[index], X_train_scaled[index]], axis=1)\n",
    "\n",
    "    X_test_scaled[index] = scaler.transform(X_test[index].loc[:, selected_columns])\n",
    "    X_test_non_scaled[index] = X_test[index][X_test[index].columns.difference(selected_columns)]\n",
    "    X_test_scaled[index] = np.concatenate([X_test_non_scaled[index], X_test_scaled[index]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train models and predict test and validation samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training:  2021-03-12 15:57:52.450292\n",
      "1/13\n",
      "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
      "                      max_depth=7, max_features='auto', max_leaf_nodes=None,\n",
      "                      max_samples=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      n_estimators=100, n_jobs=None, oob_score=False,\n",
      "                      random_state=0, verbose=0, warm_start=False)\n",
      "2/13\n",
      "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
      "                      max_depth=7, max_features='auto', max_leaf_nodes=None,\n",
      "                      max_samples=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      n_estimators=100, n_jobs=None, oob_score=False,\n",
      "                      random_state=0, verbose=0, warm_start=False)\n",
      "3/13\n",
      "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
      "                      max_depth=7, max_features='auto', max_leaf_nodes=None,\n",
      "                      max_samples=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      n_estimators=100, n_jobs=None, oob_score=False,\n",
      "                      random_state=0, verbose=0, warm_start=False)\n",
      "4/13\n",
      "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
      "                      max_depth=7, max_features='auto', max_leaf_nodes=None,\n",
      "                      max_samples=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      n_estimators=100, n_jobs=None, oob_score=False,\n",
      "                      random_state=0, verbose=0, warm_start=False)\n",
      "5/13\n",
      "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
      "                      max_depth=7, max_features='auto', max_leaf_nodes=None,\n",
      "                      max_samples=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      n_estimators=100, n_jobs=None, oob_score=False,\n",
      "                      random_state=0, verbose=0, warm_start=False)\n",
      "6/13\n",
      "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
      "                      max_depth=7, max_features='auto', max_leaf_nodes=None,\n",
      "                      max_samples=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      n_estimators=100, n_jobs=None, oob_score=False,\n",
      "                      random_state=0, verbose=0, warm_start=False)\n",
      "7/13\n",
      "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
      "                      max_depth=7, max_features='auto', max_leaf_nodes=None,\n",
      "                      max_samples=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      n_estimators=100, n_jobs=None, oob_score=False,\n",
      "                      random_state=0, verbose=0, warm_start=False)\n",
      "8/13\n",
      "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
      "                      max_depth=7, max_features='auto', max_leaf_nodes=None,\n",
      "                      max_samples=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      n_estimators=100, n_jobs=None, oob_score=False,\n",
      "                      random_state=0, verbose=0, warm_start=False)\n",
      "9/13\n",
      "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
      "                      max_depth=7, max_features='auto', max_leaf_nodes=None,\n",
      "                      max_samples=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      n_estimators=100, n_jobs=None, oob_score=False,\n",
      "                      random_state=0, verbose=0, warm_start=False)\n",
      "10/13\n",
      "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
      "                      max_depth=7, max_features='auto', max_leaf_nodes=None,\n",
      "                      max_samples=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      n_estimators=100, n_jobs=None, oob_score=False,\n",
      "                      random_state=0, verbose=0, warm_start=False)\n",
      "11/13\n",
      "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
      "                      max_depth=7, max_features='auto', max_leaf_nodes=None,\n",
      "                      max_samples=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      n_estimators=100, n_jobs=None, oob_score=False,\n",
      "                      random_state=0, verbose=0, warm_start=False)\n",
      "12/13\n",
      "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
      "                      max_depth=7, max_features='auto', max_leaf_nodes=None,\n",
      "                      max_samples=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      n_estimators=100, n_jobs=None, oob_score=False,\n",
      "                      random_state=0, verbose=0, warm_start=False)\n",
      "13/13\n",
      "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
      "                      max_depth=7, max_features='auto', max_leaf_nodes=None,\n",
      "                      max_samples=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      n_estimators=100, n_jobs=None, oob_score=False,\n",
      "                      random_state=0, verbose=0, warm_start=False)\n",
      "End training:  2021-03-12 16:02:46.596454\n"
     ]
    }
   ],
   "source": [
    "print(\"Start training: \", datetime.datetime.now())\n",
    "\n",
    "predictor = [None]*len(list_categoria_uno)\n",
    "y_train_predicted = [None]*len(list_categoria_uno)\n",
    "y_test_predicted = [None]*len(list_categoria_uno)\n",
    "\n",
    "for index in range(len(list_categoria_uno)):\n",
    "#    predictor[index] = LinearSVR(random_state=0, tol=1e-5, max_iter=10000)\n",
    "#    predictor[index] = SVR(kernel='rbf', C=2., tol=1e-5, max_iter=100000)\n",
    "    predictor[index] = RandomForestRegressor(max_depth=7, random_state=0)\n",
    "#    predictor[index] = RandomForestRegressor(max_depth=4, n_estimators=200, random_state=0)\n",
    "    print(f\"{index+1}/{len(list_categoria_uno)}\\n{predictor[index]}\")\n",
    "\n",
    "    predictor[index].fit(X_train_scaled[index], y_train[index])\n",
    "    y_train_predicted[index] = predictor[index].predict(X_train_scaled[index])\n",
    "    y_test_predicted[index] = predictor[index].predict(X_test_scaled[index])\n",
    "\n",
    "print(\"End training: \", datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrica_atmira(y_test, y_predicted):\n",
    "    rmse = mean_squared_error(y_test, y_predicted, squared=False)\n",
    "    rrmse = rmse/y_test.mean()\n",
    "    # Si el valor és negatiu és que hi ha hagut més demanda de la prevista, si el valor és positiu compta com a CF\n",
    "    diferencia = y_predicted - y_test\n",
    "    CF = np.sum(diferencia >= 0)/len(y_test)\n",
    "    metrica_minimitzar = (0.7*rrmse) + (0.3*(1-CF))\n",
    "    print(\"rmse = \", rmse)\n",
    "    print(\"y_mean = \", y_test.mean())\n",
    "    print(\"rrmse = \", rrmse)\n",
    "    print(\"CF =\", CF)\n",
    "    return metrica_minimitzar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A 515399 515399 515399 515399      \t 57267 57267 57267 57267\n",
      "B 44742 44742 560141 560141      \t 4972 4972 62239 62239\n",
      "C 142920 142920 703061 703061      \t 15880 15880 78119 78119\n",
      "D 438 438 703499 703499      \t 49 49 78168 78168\n",
      "E 126398 126398 829897 829897      \t 14045 14045 92213 92213\n",
      "F 104199 104199 934096 934096      \t 11578 11578 103791 103791\n",
      "G 73861 73861 1007957 1007957      \t 8207 8207 111998 111998\n",
      "H 117819 117819 1125776 1125776      \t 13091 13091 125089 125089\n",
      "I 27699 27699 1153475 1153475      \t 3078 3078 128167 128167\n",
      "K 203628 203628 1357103 1357103      \t 22626 22626 150793 150793\n",
      "L 40483 40483 1397586 1397586      \t 4499 4499 155292 155292\n",
      "N 3510 3510 1401096 1401096      \t 390 390 155682 155682\n",
      "O 2199 2199 1403295 1403295      \t 245 245 155927 155927\n"
     ]
    }
   ],
   "source": [
    "# Reconstruct joint y_test, y_train, y_test_predicted and y_train_predicted\n",
    "y_train_reconst = pd.Series()\n",
    "y_train_predicted_reconst = np.array([])\n",
    "\n",
    "y_test_reconst = pd.Series()\n",
    "y_test_predicted_reconst = np.array([])\n",
    "\n",
    "for index in range(len(list_categoria_uno)):\n",
    "    y_train_reconst = y_train_reconst.append(y_train[index])\n",
    "    y_train_predicted_reconst = np.concatenate((y_train_predicted_reconst,\n",
    "                                                y_train_predicted[index]))\n",
    "    \n",
    "    y_test_reconst = y_test_reconst.append(y_test[index])\n",
    "    y_test_predicted_reconst = np.concatenate((y_test_predicted_reconst,\n",
    "                                               y_test_predicted[index]))\n",
    "\n",
    "    print(list_categoria_uno[index], \n",
    "          len(y_train[index]), len(y_train_predicted[index]), \n",
    "          len(y_train_reconst), len(y_train_predicted_reconst), \"     \\t\",\n",
    "          len(y_test[index]), len(y_test_predicted[index]), \n",
    "          len(y_test_reconst), len(y_test_predicted_reconst)\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse =  10.892049990095293\n",
      "y_mean =  4.072164441546503\n",
      "rrmse =  2.674756912802562\n",
      "CF = 0.7606226773415425\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.9441430357593306"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Miro si fa sobreajust usant RFR amb 7 fulles com a màxim (miro l'error del train)\n",
    "metrica_atmira(y_train_reconst, y_train_predicted_reconst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse =  11.83258455116899\n",
      "y_mean =  4.116791832075266\n",
      "rrmse =  2.8742246472064656\n",
      "CF = 0.7612215972859094\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.083590773858753"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Això és usant RFR amb 7 fulles com a màxim (en comptes de 4) 5 minuts\n",
    "metrica_atmira(y_test_reconst, y_predicted_reconst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse =  13.48510647118722\n",
      "y_mean =  4.116791832075266\n",
      "rrmse =  3.275634771260564\n",
      "CF = 0.74172529452885\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.3704267515237394"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Això és usant one-hot encoding per la categoria_dos\n",
    "metrica_atmira(y_test_reconst, y_predicted_reconst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse =  13.368027489812341\n",
      "y_mean =  4.046418356525372\n",
      "rrmse =  3.303669149348997\n",
      "CF = 0.7432657389851207\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.3895886828487614"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Això és usant one-hot encoding per la categoria_dos i també la categoria_dos (error)\n",
    "metrica_atmira(y_test_reconst, y_predicted_reconst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse =  13.417382244806692\n",
      "y_mean =  4.046418356525372\n",
      "rrmse =  3.3158662952310483\n",
      "CF = 0.7394478902207796\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.3992720395954996"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Això és usant totes les dades per l'entrenament i el test\n",
    "metrica_atmira(y_test_reconst, y_predicted_reconst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse =  15.497067571540361\n",
      "y_mean =  4.046418356525372\n",
      "rrmse =  3.829823366273865\n",
      "CF = 0.7502972651605232\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.7557871768435485"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Això és usant totes les dades típiques per l'entrenament i totes pel test\n",
    "metrica_atmira(y_test_reconst, y_predicted_reconst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse =  11.402529733062975\n",
      "y_mean =  3.535666398498257\n",
      "rrmse =  3.225001583267612\n",
      "CF = 0.7617621644170317\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.3289724589622187"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Això és usant totes les dades i el RobustScaler\n",
    "metrica_atmira(y_test_reconst, y_predicted_reconst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse =  11.378664536251144\n",
      "y_mean =  3.535666398498257\n",
      "rrmse =  3.218251739215026\n",
      "CF = 0.7617621644170317\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.3242475681254082"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Això és usant totes les dades\n",
    "metrica_atmira(y_test_reconst, y_predicted_reconst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse =  13.198687140539754\n",
      "y_mean =  3.618860510805501\n",
      "rrmse =  3.647194220702896\n",
      "CF = 0.7670677799607073\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.6229156205038144"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Això és afegint les visites d'1 i 7 dies abans\n",
    "metrica_atmira(y_test_reconst, y_predicted_reconst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse =  13.255359153995897\n",
      "y_mean =  3.618860510805501\n",
      "rrmse =  3.662854402488551\n",
      "CF = 0.7670923379174852\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.6338703803667403"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Això és afegint les visites de 7 dies abans\n",
    "metrica_atmira(y_test_reconst, y_predicted_reconst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse =  12.35724949242855\n",
      "y_mean =  3.6659012770137522\n",
      "rrmse =  3.3708625952128153\n",
      "CF = 0.7599950884086444\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.4316052901263774"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrica_atmira(y_test_reconst, y_predicted_reconst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse =  13.424364936164661\n",
      "y_mean =  3.8621944344795778\n",
      "rrmse =  3.4758387139495643\n",
      "CF = 0.7556329968055484\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.5063972007230304"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrica_atmira(y_test_reconst, y_predicted_reconst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse =  11.67899295726826\n",
      "y_mean =  3.566853482786229\n",
      "rrmse =  3.274312503620215\n",
      "CF = 0.477982385908727\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.448624036761532"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrica_atmira(y_test_reconst, y_predicted_reconst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "144.4px",
    "left": "1166px",
    "right": "20px",
    "top": "121px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
