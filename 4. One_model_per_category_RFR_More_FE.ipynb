{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.svm import LinearSVR, SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Use this DF from here on with the filled Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muriel/.local/lib/python3.8/site-packages/numpy/lib/arraysetops.py:580: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\n",
    "    './data/Modelar_UH2021_filled_precio.txt', parse_dates=[1], index_col=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py:3062: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "df_est = pd.read_csv(\n",
    "    './data/Estimar_UH2021_filled_precio.txt', parse_dates=[1], index_col=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df=df.drop_duplicates()\n",
    "\n",
    "conditions = [ (df[\"estado\"] == 'No Rotura'), (df[\"estado\"] == 'Transito'), (df[\"estado\"] == 'Rotura') ]\n",
    "values = [1, 0, -1]\n",
    "df[\"estado_num\"] = np.select(conditions, values)\n",
    "\n",
    "df[\"weekday\"] = df[\"fecha\"].dt.weekday\n",
    "df[\"antiguedad\"] = df[\"antiguedad\"].astype('Int64')\n",
    "df[\"antiguedad_std\"] = df[\"antiguedad\"]-df[\"antiguedad\"].min()\n",
    "df[\"categoria_dos\"] = df[\"categoria_dos\"].astype(\"Int64\")\n",
    "\n",
    "df = df.drop(columns=[\"estado\", \"antiguedad\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cyclical features\n",
    "df['weekday_sin'] = np.sin(df.fecha.dt.weekday * (2*np.pi/7))\n",
    "df['weekday_cos'] = np.cos(df.fecha.dt.weekday * (2*np.pi/7))\n",
    "\n",
    "month_con = df[\"fecha\"].dt.month + (df[\"fecha\"].dt.day / df[\"fecha\"].dt.days_in_month)\n",
    "df['month_sin'] = np.sin((month_con-1) * (2*np.pi/12))\n",
    "df['month_cos'] = np.cos((month_con-1) * (2*np.pi/12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AQUÍ\n",
    "# One-hot encoding of categoria_dos\n",
    "df[\"categoria_dos\"].unique()\n",
    "df['categoria_dos_'+str()] = (df.dia_atipico == -1).astype(int)\n",
    "df['categoria_dos_'+str] = (df.dia_atipico == 0).astype(int)\n",
    "df['dia_atipico_pos'] = (df.dia_atipico == 1).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_est=df_est.drop_duplicates()\n",
    "\n",
    "conditions = [ (df_est[\"estado\"] == 'No Rotura'), (df_est[\"estado\"] == 'Transito') ]\n",
    "values = [1, 0]\n",
    "df_est[\"estado_num\"] = np.select(conditions, values)\n",
    "\n",
    "df_est[\"weekday\"] = df_est[\"fecha\"].dt.weekday\n",
    "df_est[\"antiguedad\"] = pd.to_numeric(df_est[\"antiguedad\"], errors='coerce') \n",
    "df_est[\"antiguedad\"] = df_est[\"antiguedad\"].astype('Int64')\n",
    "df_est[\"antiguedad_std\"] = df_est[\"antiguedad\"]-df_est[\"antiguedad\"].min()\n",
    "df_est[\"categoria_dos\"] = pd.to_numeric(df_est[\"categoria_dos\"], errors='coerce') \n",
    "df_est[\"categoria_dos\"] = df_est[\"categoria_dos\"].astype(\"Int64\")\n",
    "\n",
    "df_est = df_est.drop(columns=[\"estado\", \"antiguedad\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cyclical features\n",
    "df_est['weekday_sin'] = np.sin(df_est.fecha.dt.weekday * (2*np.pi/7))\n",
    "df_est['weekday_cos'] = np.cos(df_est.fecha.dt.weekday * (2*np.pi/7))\n",
    "\n",
    "month_con = df_est[\"fecha\"].dt.month + (df_est[\"fecha\"].dt.day / df_est[\"fecha\"].dt.days_in_month)\n",
    "df_est['month_sin'] = np.sin((month_con-1) * (2*np.pi/12))\n",
    "df_est['month_cos'] = np.cos((month_con-1) * (2*np.pi/12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiply x5 number of visits before 25/1/2021\n",
    "conditions = [ (df[\"fecha\"] >= datetime.datetime(2016,1,25)), (df[\"fecha\"] < datetime.datetime(2016,1,25)) ]\n",
    "values = [df[\"visitas\"], df[\"visitas\"]*5]\n",
    "df[\"visitas\"] = np.select(conditions, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (NO MILLORA) Add feature visits 1 day before \n",
    "# It only works if there are no missing entries and they are sorted (!!)\n",
    "df = df.sort_values(by = ['categoria_uno','id', 'fecha']).reset_index(drop=True)\n",
    "\n",
    "# Create transitional previous day visits feature\n",
    "first_element = pd.Series([0])\n",
    "visits_day_before = first_element.append(df[\"visitas\"], ignore_index=True)\n",
    "df[\"visitas_dia_antes\"] = visits_day_before\n",
    "\n",
    "# Select all the first entries from the stations and set the previous entries and the tendencies to 0\n",
    "product_unique_ids = sorted(df[\"id\"].unique())\n",
    "\n",
    "for product_id in product_unique_ids:\n",
    "    first_id_index = df[df[\"id\"] == product_id].index[0]\n",
    "    df.at[first_id_index, \"visitas_dia_antes\"] = 0        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (NO MILLORA) Add feature visits 7 days before \n",
    "# It only works if there are no missing entries and they are sorted (!!)\n",
    "df = df.sort_values(by = ['categoria_uno','id', 'fecha']).reset_index(drop=True)\n",
    "\n",
    "# Create transitional 7-day previous visits feature\n",
    "first_element = pd.Series([0, 0, 0, 0, 0, 0, 0])\n",
    "visits_7_days_before = first_element.append(df[\"visitas\"], ignore_index=True)\n",
    "df[\"visitas_7_dias_antes\"] = visits_7_days_before\n",
    "\n",
    "# Select all the first entries from the stations and set the previous entries and the tendencies to 0\n",
    "product_unique_ids = sorted(df[\"id\"].unique())\n",
    "\n",
    "for product_id in product_unique_ids:\n",
    "    first_id_index = df[df[\"id\"] == product_id].index[0]\n",
    "    for extra_day in range(7):\n",
    "        df.at[first_id_index+extra_day, \"visitas_7_dias_antes\"] = 0        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only use data before the pattern change\n",
    "#df = df[df.fecha < datetime.datetime(2016,1,24)]\n",
    "#df_est = df_est[df_est.fecha < datetime.datetime(2016,1,24)]\n",
    "# Only use data after the pattern change\n",
    "#df = df[df.fecha > datetime.datetime(2016,1,25)]\n",
    "#df_est = df_est[df_est.fecha > datetime.datetime(2016,1,25)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only use data from typical days outside of campaigns\n",
    "df = df.loc[(df[\"dia_atipico\"] == 0) & (df[\"campaña\"] == 0), ]\n",
    "df_est = df_est.loc[(df_est[\"dia_atipico\"] == 0) & (df_est[\"campaña\"] == 0), ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unwanted columns\n",
    "df = df.drop(columns=[\"fecha\", \"id\", \"weekday\"])\n",
    "df_est = df_est.drop(columns=[\"fecha\", \"id\", \"weekday\"])\n",
    "#df = df.drop(columns=[\"fecha\", \"id\", \"weekday_sin\", \"weekday_cos\"])\n",
    "#df_est = df_est.drop(columns=[\"fecha\", \"id\", \"weekday_sin\", \"weekday_cos\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop nans\n",
    "df = df.dropna()\n",
    "df_est = df_est.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset in categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only 492895 samples for categoria_uno = A\n",
      "Only 43044 samples for categoria_uno = B\n",
      "Only 137572 samples for categoria_uno = C\n",
      "Only 422 samples for categoria_uno = D\n",
      "Only 119848 samples for categoria_uno = E\n",
      "Only 98746 samples for categoria_uno = F\n",
      "Only 70896 samples for categoria_uno = G\n",
      "Only 112674 samples for categoria_uno = H\n",
      "Only 26586 samples for categoria_uno = I\n",
      "Only 195386 samples for categoria_uno = K\n",
      "Only 38824 samples for categoria_uno = L\n",
      "Only 3376 samples for categoria_uno = N\n",
      "Only 2110 samples for categoria_uno = O\n"
     ]
    }
   ],
   "source": [
    "# Split dataset in categorias_uno and limit number of training samples per categoria_uno\n",
    "number_samples_desired = 1000000\n",
    "\n",
    "list_categoria_uno = sorted( df[\"categoria_uno\"].unique() )\n",
    "data_cat = [None]*len(list_categoria_uno)\n",
    "data_est_cat = [None]*len(list_categoria_uno)\n",
    "\n",
    "for index in range(len(list_categoria_uno)):\n",
    "\n",
    "    number_samples = number_samples_desired\n",
    "    number_samples_available = len(df[df[\"categoria_uno\"] == list_categoria_uno[index]])\n",
    "    if number_samples_desired > number_samples_available:\n",
    "        number_samples = number_samples_available\n",
    "        print(f\"Only {number_samples_available} samples for categoria_uno = {list_categoria_uno[index]}\")\n",
    "        \n",
    "    # Modelar\n",
    "    data_cat[index] = df[df[\"categoria_uno\"] == list_categoria_uno[index]].sample(n=number_samples, random_state=0)\n",
    "    data_cat[index] = data_cat[index].drop(columns = \"categoria_uno\")\n",
    "    # Estimar\n",
    "    data_est_cat[index] = df_est[df_est[\"categoria_uno\"] == list_categoria_uno[index]]\n",
    "    data_est_cat[index] = data_est_cat[index].drop(columns = \"categoria_uno\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(492895, 11) (492895,)\n",
      "443605 443605 49290\n",
      "(443605, 11) (443605,) (49290, 11)\n"
     ]
    }
   ],
   "source": [
    "X = [None]*len(list_categoria_uno)\n",
    "y = [None]*len(list_categoria_uno)\n",
    "X_train = [None]*len(list_categoria_uno)\n",
    "X_test = [None]*len(list_categoria_uno)\n",
    "y_train = [None]*len(list_categoria_uno)\n",
    "y_test = [None]*len(list_categoria_uno)\n",
    "\n",
    "for index in range(len(list_categoria_uno)):\n",
    "    X[index] = data_cat[index][data_cat[index].columns.difference([\"unidades_vendidas\"])]\n",
    "    y[index] = data_cat[index][\"unidades_vendidas\"]\n",
    "\n",
    "    X_train[index], X_test[index], y_train[index], y_test[index] = train_test_split(\n",
    "                                                            X[index], y[index], test_size=0.10, random_state=0)\n",
    "\n",
    "print(X[0].shape, y[0].shape)\n",
    "print(len(X_train[0]), len(y_train[0]), len(X_test[0]))\n",
    "print(X_train[0].shape, y_train[0].shape, X_test[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalitzation of selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize some features of the train and test datasets with the Standard Scaler/Robust Scaler \n",
    "# Select which columns to use with the scaler\n",
    "\n",
    "selected_columns = [\n",
    "#    \"fecha\",\n",
    "#    \"id\",\n",
    "    \"antiguedad_std\",\n",
    "#    \"campaña\",\n",
    "#    \"categoria_uno\",\n",
    "    \"categoria_dos\",\n",
    "#    \"dia_atipico\",\n",
    "#    \"estado_num\",\n",
    "#    \"month_cos\",\n",
    "#    \"month_sin\",\n",
    "    \"precio\",\n",
    "    \"visitas\",\n",
    "#    \"weekday\",\n",
    "#    \"weekday_cos\",\n",
    "#    \"weekday_sin\",\n",
    "#    \"visitas_dia_antes\",\n",
    "#    \"visitas_7_dias_antes\",\n",
    "    ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I train the scaler with ALL training data available\n",
    "scaler = StandardScaler().fit(df.loc[:,selected_columns])\n",
    "#scaler = RobustScaler().fit(df.loc[:,selected_columns])\n",
    "\n",
    "X_train_scaled = [None]*len(list_categoria_uno)\n",
    "X_train_non_scaled = [None]*len(list_categoria_uno)\n",
    "X_test_scaled = [None]*len(list_categoria_uno)\n",
    "X_test_non_scaled = [None]*len(list_categoria_uno)\n",
    "\n",
    "for index in range(len(list_categoria_uno)):\n",
    "    X_train_scaled[index] = scaler.transform(X_train[index].loc[:, selected_columns])\n",
    "    X_train_non_scaled[index] = X_train[index][X_train[index].columns.difference(selected_columns)]\n",
    "    X_train_scaled[index] = np.concatenate([X_train_non_scaled[index], X_train_scaled[index]], axis=1)\n",
    "\n",
    "    X_test_scaled[index] = scaler.transform(X_test[index].loc[:, selected_columns])\n",
    "    X_test_non_scaled[index] = X_test[index][X_test[index].columns.difference(selected_columns)]\n",
    "    X_test_scaled[index] = np.concatenate([X_test_non_scaled[index], X_test_scaled[index]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train models and predict validation samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training:  2021-03-11 15:25:32.131712\n",
      "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
      "                      max_depth=4, max_features='auto', max_leaf_nodes=None,\n",
      "                      max_samples=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      n_estimators=100, n_jobs=None, oob_score=False,\n",
      "                      random_state=0, verbose=0, warm_start=False)\n",
      "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
      "                      max_depth=4, max_features='auto', max_leaf_nodes=None,\n",
      "                      max_samples=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      n_estimators=100, n_jobs=None, oob_score=False,\n",
      "                      random_state=0, verbose=0, warm_start=False)\n",
      "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
      "                      max_depth=4, max_features='auto', max_leaf_nodes=None,\n",
      "                      max_samples=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      n_estimators=100, n_jobs=None, oob_score=False,\n",
      "                      random_state=0, verbose=0, warm_start=False)\n",
      "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
      "                      max_depth=4, max_features='auto', max_leaf_nodes=None,\n",
      "                      max_samples=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      n_estimators=100, n_jobs=None, oob_score=False,\n",
      "                      random_state=0, verbose=0, warm_start=False)\n",
      "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
      "                      max_depth=4, max_features='auto', max_leaf_nodes=None,\n",
      "                      max_samples=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      n_estimators=100, n_jobs=None, oob_score=False,\n",
      "                      random_state=0, verbose=0, warm_start=False)\n",
      "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
      "                      max_depth=4, max_features='auto', max_leaf_nodes=None,\n",
      "                      max_samples=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      n_estimators=100, n_jobs=None, oob_score=False,\n",
      "                      random_state=0, verbose=0, warm_start=False)\n",
      "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
      "                      max_depth=4, max_features='auto', max_leaf_nodes=None,\n",
      "                      max_samples=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      n_estimators=100, n_jobs=None, oob_score=False,\n",
      "                      random_state=0, verbose=0, warm_start=False)\n",
      "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
      "                      max_depth=4, max_features='auto', max_leaf_nodes=None,\n",
      "                      max_samples=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      n_estimators=100, n_jobs=None, oob_score=False,\n",
      "                      random_state=0, verbose=0, warm_start=False)\n",
      "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
      "                      max_depth=4, max_features='auto', max_leaf_nodes=None,\n",
      "                      max_samples=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      n_estimators=100, n_jobs=None, oob_score=False,\n",
      "                      random_state=0, verbose=0, warm_start=False)\n",
      "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
      "                      max_depth=4, max_features='auto', max_leaf_nodes=None,\n",
      "                      max_samples=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      n_estimators=100, n_jobs=None, oob_score=False,\n",
      "                      random_state=0, verbose=0, warm_start=False)\n",
      "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
      "                      max_depth=4, max_features='auto', max_leaf_nodes=None,\n",
      "                      max_samples=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      n_estimators=100, n_jobs=None, oob_score=False,\n",
      "                      random_state=0, verbose=0, warm_start=False)\n",
      "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
      "                      max_depth=4, max_features='auto', max_leaf_nodes=None,\n",
      "                      max_samples=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      n_estimators=100, n_jobs=None, oob_score=False,\n",
      "                      random_state=0, verbose=0, warm_start=False)\n",
      "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
      "                      max_depth=4, max_features='auto', max_leaf_nodes=None,\n",
      "                      max_samples=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      n_estimators=100, n_jobs=None, oob_score=False,\n",
      "                      random_state=0, verbose=0, warm_start=False)\n",
      "End training:  2021-03-11 15:27:06.187950\n"
     ]
    }
   ],
   "source": [
    "print(\"Start training: \", datetime.datetime.now())\n",
    "\n",
    "predictor = [None]*len(list_categoria_uno)\n",
    "y_predicted = [None]*len(list_categoria_uno)\n",
    "\n",
    "for index in range(len(list_categoria_uno)):\n",
    "#    predictor[index] = LinearSVR(random_state=0, tol=1e-5, max_iter=10000)\n",
    "#    predictor[index] = SVR(kernel='rbf', C=2., tol=1e-5, max_iter=100000)\n",
    "    predictor[index] = RandomForestRegressor(max_depth=4, random_state=0)\n",
    "#    predictor[index] = RandomForestRegressor(max_depth=4, n_estimators=200, random_state=0)\n",
    "    print(predictor[index])\n",
    "\n",
    "    predictor[index].fit(X_train_scaled[index], y_train[index])\n",
    "    y_predicted[index] = predictor[index].predict(X_test_scaled[index])\n",
    "\n",
    "print(\"End training: \", datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrica_atmira(y_test, y_predicted):\n",
    "    rmse = mean_squared_error(y_test, y_predicted, squared=False)\n",
    "    rrmse = rmse/y_test.mean()\n",
    "    # Si el valor és negatiu és que hi ha hagut més demanda de la prevista, si el valor és positiu compta com a CF\n",
    "    diferencia = y_predicted - y_test\n",
    "    CF = np.sum(diferencia >= 0)/len(y_test)\n",
    "    metrica_minimitzar = (0.7*rrmse) + (0.3*(1-CF))\n",
    "    print(\"rmse = \", rmse)\n",
    "    print(\"y_mean = \", y_test.mean())\n",
    "    print(\"rrmse = \", rrmse)\n",
    "    print(\"CF =\", CF)\n",
    "    return metrica_minimitzar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A 49290 49290 49290 49290\n",
      "B 4305 4305 53595 53595\n",
      "C 13758 13758 67353 67353\n",
      "D 43 43 67396 67396\n",
      "E 11985 11985 79381 79381\n",
      "F 9875 9875 89256 89256\n",
      "G 7090 7090 96346 96346\n",
      "H 11268 11268 107614 107614\n",
      "I 2659 2659 110273 110273\n",
      "K 19539 19539 129812 129812\n",
      "L 3883 3883 133695 133695\n",
      "N 338 338 134033 134033\n",
      "O 211 211 134244 134244\n"
     ]
    }
   ],
   "source": [
    "# Reconstruct joint y_test and y_predicted\n",
    "y_test_reconst = pd.Series()\n",
    "y_predicted_reconst = np.array([])\n",
    "for index in range(len(list_categoria_uno)):\n",
    "    y_test_reconst = y_test_reconst.append(y_test[index])\n",
    "    y_predicted_reconst = np.concatenate((y_predicted_reconst,y_predicted[index]))\n",
    "    print(list_categoria_uno[index], len(y_test[index]), len(y_predicted[index]), \n",
    "          len(y_test_reconst), len(y_predicted_reconst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse =  11.402529733062975\n",
      "y_mean =  3.535666398498257\n",
      "rrmse =  3.225001583267612\n",
      "CF = 0.7617621644170317\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.3289724589622187"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Això és usant totes les dades i el RobustScaler\n",
    "metrica_atmira(y_test_reconst, y_predicted_reconst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse =  11.378664536251144\n",
      "y_mean =  3.535666398498257\n",
      "rrmse =  3.218251739215026\n",
      "CF = 0.7617621644170317\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.3242475681254082"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Això és usant totes les dades\n",
    "metrica_atmira(y_test_reconst, y_predicted_reconst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse =  9.663112763713858\n",
      "y_mean =  3.2140452017754195\n",
      "rrmse =  3.006526715422674\n",
      "CF = 0.7698459262282537\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.1736149229273956"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Això és eliminant els dies atípics i campanyes\n",
    "metrica_atmira(y_test_reconst, y_predicted_reconst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse =  11.867271050707602\n",
      "y_mean =  3.2308722435496833\n",
      "rrmse =  3.673085828262064\n",
      "CF = 0.7682094710931422\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.640697238455502"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Això és eliminant els dies atípics i posant les variables visites 1 i 7 dies abans\n",
    "metrica_atmira(y_test_reconst, y_predicted_reconst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse =  9.834973478745413\n",
      "y_mean =  3.206614205364317\n",
      "rrmse =  3.0670897241996156\n",
      "CF = 0.7687833994413764\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.2163277871073177"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Això és eliminant els dies atípics\n",
    "metrica_atmira(y_test_reconst, y_predicted_reconst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse =  13.198687140539754\n",
      "y_mean =  3.618860510805501\n",
      "rrmse =  3.647194220702896\n",
      "CF = 0.7670677799607073\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.6229156205038144"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Això és afegint les visites d'1 i 7 dies abans\n",
    "metrica_atmira(y_test_reconst, y_predicted_reconst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse =  13.255359153995897\n",
      "y_mean =  3.618860510805501\n",
      "rrmse =  3.662854402488551\n",
      "CF = 0.7670923379174852\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.6338703803667403"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Això és afegint les visites de 7 dies abans\n",
    "metrica_atmira(y_test_reconst, y_predicted_reconst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse =  12.35724949242855\n",
      "y_mean =  3.6659012770137522\n",
      "rrmse =  3.3708625952128153\n",
      "CF = 0.7599950884086444\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.4316052901263774"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrica_atmira(y_test_reconst, y_predicted_reconst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse =  13.424364936164661\n",
      "y_mean =  3.8621944344795778\n",
      "rrmse =  3.4758387139495643\n",
      "CF = 0.7556329968055484\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.5063972007230304"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrica_atmira(y_test_reconst, y_predicted_reconst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse =  11.67899295726826\n",
      "y_mean =  3.566853482786229\n",
      "rrmse =  3.274312503620215\n",
      "CF = 0.477982385908727\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.448624036761532"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrica_atmira(y_test_reconst, y_predicted_reconst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "144.4px",
    "left": "1166px",
    "right": "20px",
    "top": "121px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
